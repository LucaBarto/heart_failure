---
title: "Heart failure classification"
author: "Luca Bartolomei"
date: "_`r format(Sys.Date(), '%d %B, %Y')`_"
output:
  pdf_document:
    df_print: kable
    number_sections: yes
    toc: yes
    toc_depth: 3
    fig_caption: yes
    extra_dependencies: subfig
    highlight: tango
classoption: a4paper
fig_height: 5
fig_width: 5
fontsize: 10pt
latex_engine: xelatex
mainfont: Arial
mathfont: LiberationMono
monofont: DejaVu Sans Mono
include-before: '`\newpage{}`{=latex}'
urlcolor: blue
---

\newpage

# **Introduction**

This project has the aim to analyze the Heart failure dataset to build a 
classifiers to predict whether people have heart disease or not.

The dataset used for project comes from Kaggle website:
https://www.kaggle.com/datasets/andrewmvd/heart-failure-clinical-data


Attribute Information:

1. age: The person’s age in years
2. anaemia: Decrease of red blood cells or hemoglobin
3. creatinine_phosphokinase: Level of the CPK enzyme in the blood (mcg/L)
4. diabetes: If the patient has diabetes
5. ejection_fraction: Percentage of blood leaving the heart at each contraction
6. high_blood_pressure: If the patient has hypertension
7. platelets: Platelets in the blood (kiloplatelets/mL)
8. serum_creatinine: Level of serum creatinine in the blood (mg/dL)
9. serum_sodium: Level of serum sodium in the blood (mEq/L)
10. sex: The person’s sex 
11. smoking: If person smoking
14. target: Death 
    * 0 = no
    * 1 = yes

In this project "target" attribute is going to be the dependent variable. The others
attributes are going to be the predictors.

This project will make a comparison between different machine learning algorithms 
in order to to assess the correctness in ***classifying*** data with respect 
to efficiency of each algorithm in terms of ***sensitivity*** and ***specificity***.

***Sensitivity*** and ***specificity*** are widely used in medicine for binary 
classification test.

We are going to project a ***Machine Learning Classifiers***, more 
specifically a ***Binary classifiers*** where the two classes are:

1. yes: Death
2. no: No Death


\newpage


# **Methods**

## Key concepts

***Classification***
Classification is the process of predicting the class of given data points. 
When there are only two classes the problem is known as ***statistical binary classification***.

***Accuracy*** and ***Kappa***
These are the default metrics used to evaluate algorithms on binary and 
multi-class classification datasets in caret.

***Accuracy*** is the percentage of correctly classifies instances out of all 
instances. It is more useful on a binary classification than multi-class 
classification problems because it can be less clear exactly how the accuracy 
breaks down across those classes.

***Sensitivity*** is the true positive rate also called the recall. 
It is the number instances from the positive (first) class that actually 
predicted correctly.

***Specificity*** is also called the true negative rate. Is the number of 
instances from the negative class (second) class that were actually 
predicted correctly

***Confusion Matrix***
Confusion Matrix is represented by the following table

<center>

![Confusion matrix](Confusion matrix.png)

</center>


1. TP: True positive
2. FP: False positive
3. FN: False negative
4. TN: True negative

A true positive is an outcome where the model correctly predicts 
the positive class.

A false positive is an outcome where the model incorrectly predicts 
the positive class.

A false negative is an outcome where the model incorrectly predicts 
the negative class.

A  true negative is an outcome where the model correctly predicts 
the negative class.

Now we consider these rates

$$
\mbox{TPR} = \frac{TP}{TP+FN}
$$

$$
\mbox{FPR} = \frac{FP}{TN+FP}
$$

TPR (True Positive Rate, that correspond to Sensitivity) means how often does 
the classifier correctly predict positive.

FPR (False Positive Rate) means how often does the classifier incorrectly 
predict positive. FPR also correspond to: 100% - Specificity, where Specificity 
is the proportion of true negative who are correctly identified.

***ROC*** (Receiver Operating Characteristic) Curve is a way to visualize
the performance of a binary classifier.

The ROC metric is strictly connected with Confusion Matrix

The ROC metric is better explained by ROC curve where TPR is on y axis and 
TFR on x axis.

<center>

![ROC curve](ROC curve.png)

</center>

The dotted line correspond to the random classification, so a good classification
is showed by any curve (like the blue one) up the dotted line.

The area under the ROC curve is also called: ***AUC*** (Area Under Curve)

***Covariance*** of two statistical variables is a number that provides a 
measure of how much the two vary together.

***Covariance matrix*** is a generalization of ***covariance*** to the case of 
dimension greater than two.

***Homoscedasticity***
Homoscedasticity describes a situation in which the error term is the same 
across all values of the independent variables.

***Normal distribution***
The normal distribution, also known as the bell curve and as the Gaussian 
distribution, is one of the most famous mathematical concepts in history. 
A reason for this is that approximately normal distributions occur in many 
situations, including gambling winnings, heights, weights, blood pressure, 
standardized test scores, and experimental measurement errors.

The fact that the distribution is defined by just two parameters implies that 
if a dataset is approximated by a normal distribution, all the information 
needed to describe the distribution can be encoded in just two numbers: 
the average and the standard deviation. 
We now define these values for an arbitrary list of numbers.

***Shapiro-Wilk test***
The Shapiro–Wilk test is a test of normality in frequentist statistics. 
It was published in 1965 by Samuel Sanford Shapiro and Martin Wilk.
The null-hypothesis of this test is that the population is normally distributed. 
Thus, if the p value is less than the chosen alpha level, then the null 
hypothesis is rejected and there is evidence that the data tested are not 
normally distributed.

***Supervised learning*** is a machine learning technique that aims to train a 
computer system in order to allow it to autonomously make predictions on the 
output values of a system versus an input based on a series of ideal examples, 
consisting of pairs input and output, which are initially provided.

***Unsupervised learning*** is a type of machine learning that looks for 
previously undetected patterns in a dataset with no pre-existing labels and 
with a minimum of human supervision. Unlike supervised learning which usually 
makes use of human-labeled data, unsupervised learning allows for the modeling 
of probability densities on inputs.

***The Bayes theorem*** describes the probability of an event, based on 
the preliminary knowledge of the conditions that could be related to the event.
It serves as a way to understand ***conditional probability***.

***The conditional probability*** of an event *A* with respect to an event *B* 
is the probability that *A* will occur, knowing that *B* has occurred. 
This probability, indicated ${\ P (A | B)}$ 
or ${\ P_ {B} (A)}$, 
expresses a "correction" of expectations for *A*, dictated by observation of *B*.

***Naive Bayes*** is a ***supervised learning algorithm*** suitable for solving 
binary (two-class) and multi-class classification problems. The main peculiarity 
of the algorithm, in addition to making use of ***the Bayes theorem***, is that 
it is based on the fact that all the characteristics are not related to each other.

***Ensemble model*** is a combination of single simple models 
(called weak learners) that together create a new, more powerful model 
(called strong learner).

***Boosting*** is an ***ensemble method*** for improving model predictions of a given 
learning algorithm. The idea behind it is to train weak learners sequentially, 
each trying to correct its predecessor.

***Loss function*** is a method for estimating the quality of an algorithm in 
modeling the data provided. If the forecast deviates too much from actual 
results, the loss function produces a very large number.

***Bagging*** is an ***ensemble model***. Is a general procedure that can be 
used to reduce the variance of those algorithms that have a high variance.

***Collinearity*** is a linear association between two predictors. 
***Multicollinearity*** is a situation where two or more predictors are highly 
linearly related that it can lead to misleading results when attempting 
to predict the dependent variable.

\newpage

## Model evaluation

### Metrics To Evaluate Machine Learning Algorithms

The metric used for these project is ROC.

AUC is the area under the ROC curve and represents a models ability to 
discriminate between positive and negative classes. 
An area of 1.0 represents a model that made all predicts perfectly. 
An area of 0.5 represents a model as good as random. 

ROC can be broken down into sensitivity and specificity. 
A binary classification problem is really a trade-off between sensitivity 
and specificity.

To calculate ROC information, in our trainControl we must set the summaryFunction  
to twoClassSummary. 


### Principal component analysis

Principal component analysis (PCA) is a technique that, starting from a set of 
numerical variables, obtains a smaller set of "artificial" orthogonal variables.
The reduced set of linear orthogonal projections 
(known as "principal components" or "principal components", "PC") 
is obtained by linearly combining the original variables 
in an appropriate manner.

In PCA, the term “information” indicates the total variability of the original 
input variables, ie the sum of the variances of the original variables.
The central point of the PCA is the so-called spectral decomposition 
(also called the decomposition into eigenvalues and eigenvectors, 
or eigendecomposition) of the sample variance/covariance matrix. 
This decomposition returns the eigenvalues and eigenvalues of 
the ***covariance matrix***. 
The eigenvalues (in decreasing order of value) represent the amount 
of the total variability observed on the original variables, 
explained by each main component; 
the eigenvectors instead represent the corresponding (orthogonal) 
directions of maximum variability extracted from the principal components.

The hope in the application of the PCA is that the sample variances of 
the first Main Components (PC) are large, while the variances of 
the other components are small enough to consider the corresponding 
PCs negligible. 
A principal component variable that has little variability 
(relative to other variables) can be treated roughly as a constant. 
Omitting PCs with low sample variability and placing all attention on PCs 
with higher variance can be seen as a simple and "sensible" way to reduce 
the dimensionality (number of columns) of the dataset.



### Adaptive Boosting

AdaBoost, acronym for “Adaptive Boosting” is a ***supervised learning algorithm***
proposed by Freund and Schapire in 1996. It was the first highly successful 
***boosting*** algorithm developed for ***binary classification***. 

It represents a popular boosting technique that combines multiple 
"weak classifiers" into one "strong classifier".


### Support Vector Networks
Support Vector Networks or SVM are classification algorithms used in supervised 
learning to analyze labeled training data. 
SVM can classify features in a training set into categories that use either 
a linear or non-linear model. The linearity of the classifier is determined by 
the kernel function of the data set. 
For non-linear classification in SVM using the kernel trick.


### Stochastic Gradient Boosting

Stochastic Gradient Boosting is another ***supervised learning algorithm*** for 
regression and classification problems.

It represents an ***ensemble model***. In each training cycle, or iteration, 
the weak learner is built and its predictions are compared with the correct 
result we expect.

The distance between observation and prediction represents 
the error rate of our model. These errors are defined using a ***loss function***.

The purpose of the algorithm is to minimize this ***loss function***, using a 
tool that is called "***gradient***", which basically represents the 
partial derivative of the ***loss function***.


### Classification and Regression Trees

Classification Trees is another ***supervised learning algorithm*** for 
regression and classification problems. Is based on concept of ***decision tree***.

A tree is a collection of entities called nodes connected to each other by 
arrows or lines. 
Each node contains a value and may or may not have child nodes, while arrows 
indicate the decisions/rules a tree may or may not make.

<center>

![Decision tree](Decision tree.png)

</center>

The nodes are A, B, C, D, E and F and are connected by lines that indicate 
kinship relations between the various nodes.

Node A is called the root node and is the starting point of the tree. 
It consists of three children: node B, node C and node D. The only node without
a parent is the root node.

A tree is a ***binary tree*** when each parent has at most 2 child nodes.
Decision trees are ***binary trees***.

An example of a binary tree is shown in the following image.

\newpage

<center>

![Binary tree](Binary tree.png)

</center>

***Classification and Regression Trees***, also called CART, it is basically a 
binary tree.


### Random Forest

Random Forest is another ***supervised learning algorithm***

It represents a type of ***ensemble model***, which uses ***bagging*** as an 
ensemble method and the decision tree as an individual model.

This means that a random forest combines many decision trees into one model. 
Individually, the predictions made by the decision trees may not be accurate, 
but combined together, the predictions will on average be closer to the outcome.


### K Nearest Neighbor (KNN)

KNN is a ***supervised learning algorithm***, whose purpose is to predict a new 
instance by knowing the data points that are separated into different classes.

Its operation is based on the similarity of characteristics: the closer an 
instance is to a data point, the more the knn will consider them similar.

Similarity is usually calculated by Euclidean distance. 
The shorter the distance, the greater the similarity between the data point 
and the instance to be predicted.

In addition to the distance, the algorithm provides for setting a parameter k, 
chosen arbitrarily, which identifies the number of closest data points. 
The algorithm evaluates the k minimum distances thus obtained. 
The class that obtains the greatest number of these distances 
is chosen as the prediction.


### Neural Network

A neural network is a model composed of artificial "neurons", vaguely inspired 
by the simplification of a biological neural network.

This model consists of a group of information interconnections made up of 
artificial and computational neurons processes. In most cases, an artificial 
neural network is an adaptive system that changes its own structure based on 
external or internal information flowing through the network itself during 
learning phase.

An artificial neural network receives external signals on a layer of input 
nodes (processing units), each of which is connected with numerous internal 
nodes, organized in several layers. 
Each node processes the received signals and transmits the result 
to subsequent nodes.

A neural Network model it can be both supervised and unsupervised. 
In this project we will use the ***supervised model***. 

\newpage

## Data exploration

```{r Install libraries,echo=FALSE,message=FALSE,warning=FALSE}
# Install all the necessary libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("corrplot", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")
if(!require(funModeling)) install.packages("funModeling", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(dplyr)
library(ggplot2)
library(corrplot)
library(knitr)
library(funModeling)
library(caret)
library(ggthemes)
```


```{r Load data,echo=FALSE,message=FALSE,warning=FALSE}
# Read data from file and save in a data frame
data <- read.csv('https://raw.githubusercontent.com/LucaBarto/heart_failure/main/heart_failure_clinical_records_dataset.csv')
```


Let's explore the data structure

```{r Explore data,echo=FALSE,message=FALSE,warning=FALSE}
# Explore structure
str(data)

# Summary of data
summary(data)
```


```{r Find missing value and duplicated,echo=FALSE,message=FALSE,warning=FALSE}

# Change column name
colnames(data)[13] <- "target"

data <- data %>% dplyr::select(-time)

# Set all numeric outputs to 3 digits
options(digits = 3)

# Check for missing values
missing_value <- map_int(data, function(.x) sum(is.na(.x)))

# Check for duplicate
duplicated_value <- sum(duplicated(data))

```

From the analysis of the observations we can understand that there are no missing values or duplicated observations

\newpage

### Correlation between predictors

We analyze the correlation between variables through the use of graphs showing the correlation matrix

```{r Find correlation between predictors,echo=FALSE,message=FALSE,warning=FALSE}
# Find correlation between predictors

# Correlation matrix
correlationMatrix <- cor(data[,1:ncol(data) - 1])

# The corrplot package is a graphical display of a correlation matrix, 
# confidence interval or general matrix
corrplot(correlationMatrix, order = "hclust", tl.cex = 1, addrect = 8)

# Find attributes that are highly corrected
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.6)

```

The correlation is below 0.60, therefore well below the cut-off of 0.70 
below which it can be said that it does not exist high ***collinearity*** between
predictors, but it is not enough to be confident in using ***Naive Bayes*** algorithm.

```{r Change target variable to factor,echo=FALSE,message=FALSE,warning=FALSE}
# Change target variable to factor
data$target <- as.factor(ifelse(data$target == 1, "yes", "no"))
```

The percentages of observations for the two possible target values are as follows:

1. No: 67.9% 
2. Yes: 32.1%

Check proportion of data
```{r Check proportion of data,echo=FALSE,message=FALSE,warning=FALSE}
# Check proportion of data
prop.table(table(data$target))
```

\newpage

### Data distribution

We investigate the individual variables to understand if they are normally distributed.
For this purpose, we use graphs and then implement the Shapiro-Wilk test.

```{r Check Normal distribution,echo=FALSE,message=FALSE,warning=FALSE}
qqplot_normal_detect <- function(x, yLab) {
  #Let's construct a QQ-plot using R code. Start by defining the vector of proportions.
  p <- seq(0.05, 0.95, 0.05)
  
  #To obtain the quantiles from the data, we can use the quantile function like this:
  sample_quantiles <- quantile(x, p)
  
  #To obtain the theoretical normal distribution quantiles with the corresponding average and SD, we use the qnorm function:
  theoretical_quantiles <- qnorm(p, mean = mean(x), sd = sd(x))
  
  #To see if they match or not, we plot them against each other and draw the identity line:
  qplot(theoretical_quantiles, sample_quantiles) + geom_abline() + 
    labs(x = "Theoretical normal distribution") +
    labs(y = paste(yLab, " quantiles"))
}

qqplot_normal_detect(data$age, 'Age')

qqplot_normal_detect(data$anaemia, 'Anaemia')

qqplot_normal_detect(data$creatinine_phosphokinase, 'Creatinine phosphokinase')

qqplot_normal_detect(data$diabetes, 'Diabetes')

qqplot_normal_detect(data$ejection_fraction, 'Ejection fraction')

qqplot_normal_detect(data$high_blood_pressure, 'High blood pressure')

qqplot_normal_detect(data$platelets, 'Platelets')

qqplot_normal_detect(data$serum_creatinine, 'Serum creatinine')

qqplot_normal_detect(data$serum_sodium, 'Serum sodium')

qqplot_normal_detect(data$sex, 'Sex')

qqplot_normal_detect(data$smoking, 'Smoking')
```

Only Age and Serum sodium they appear to have a normal distribution

To understand how much each variable differs from the normal distribution we implement the Shapiro-Wilk test

```{r Shapiro-Wilk test,echo=FALSE,message=FALSE,warning=FALSE}
shapiro_age <- shapiro.test(data$age)

shapiro_anaemia <- shapiro.test(data$anaemia)

shapiro_creatinine_phosphokinase <- shapiro.test(data$creatinine_phosphokinase)

shapiro_diabetes <- shapiro.test(data$diabetes)

shapiro_ejection_fraction <- shapiro.test(data$ejection_fraction)

shapiro_high_blood_pressure <- shapiro.test(data$high_blood_pressure)

shapiro_platelets <- shapiro.test(data$platelets)

shapiro_serum_creatinine <- shapiro.test(data$serum_creatinine)

shapiro_serum_sodium <- shapiro.test(data$serum_sodium)

shapiro_sex <- shapiro.test(data$sex)

shapiro_smoking <- shapiro.test(data$smoking)

variables <- c('age', 'anaemia', 'creatinine phosphokinase', 'diabetes', 
                  'ejection fraction', 'high blood pressure', 'platelets', 
                  'serum creatinine', 'serum sodium', 'sex', 'smoking')

pvalues <- c(shapiro_age$p.value, shapiro_anaemia$p.value, shapiro_creatinine_phosphokinase$p.value,
             shapiro_diabetes$p.value, shapiro_ejection_fraction$p.value,
             shapiro_high_blood_pressure$p.value, shapiro_platelets$p.value,
             shapiro_serum_creatinine$p.value, shapiro_serum_sodium$p.value,
             shapiro_sex$p.value, shapiro_smoking$p.value)

df_shapiro <- data.frame(variables,pvalues)

colnames(df_shapiro) <- c('Variable', 'P-VALUE')

df_shapiro %>% kable(digits = 40)

```

By analyzing the p-value we understand that no variable follow a normal distribution

\newpage

### Homoscedasticity analysis

Now we want to understand if is respected the hypothesis of homoscedasticity

```{r Check Homoscedasticity,echo=FALSE,message=FALSE,warning=FALSE}
homoscedasticity_test <- function(x, y, data, title) {
  lmMod <- lm(x ~ y, data=data) # initial model
  
  par(mfrow=c(2,2)) # init 4 charts in 1 panel
  plot(lmMod, caption = title, cex.caption = 1.2)
}

homoscedasticity_test(data$age, data$target, data, 'Age')
homoscedasticity_test(data$anaemia, data$target, data, 'Anaemia')
homoscedasticity_test(data$creatinine_phosphokinase, data$target, data, 'Creatinine phosphokinase')
homoscedasticity_test(data$diabetes, data$target, data, 'Diabetes')
homoscedasticity_test(data$ejection_fraction, data$target, data, 'Ejection fraction')
homoscedasticity_test(data$high_blood_pressure, data$target, data, 'High blood pressure')
homoscedasticity_test(data$platelets, data$target, data, 'Platelets')
homoscedasticity_test(data$serum_creatinine, data$target, data, 'Serum creatinine')
homoscedasticity_test(data$serum_sodium, data$target, data, 'Serum sodium')
homoscedasticity_test(data$sex, data$target, data, 'Sex')
homoscedasticity_test(data$smoking, data$target, data, 'Smoking')

par(mfrow=c(1,1))

```

By analyzing the plot of Residuals (top-left) and the one of square root of Standard residuals (bottom-left), we see that no variable respects the hypothesis of homoscedasticity

\newpage

## Visualization

The next graphs show the following distributions:

1. Distribution of target 
2. Distribution of variables
3. Distribution of variables for each target value

Distribution of target

```{r Distribution of target,echo=FALSE,message=FALSE,warning=FALSE}
# Plot distribution of target
ggplot(data, aes(x=target)) +
  geom_bar(fill="blue",alpha=0.5) +
  theme_economist() +
  labs(title="Distribution of target")
```

\newpage

Distribution of variables

```{r Data distribution,echo=FALSE,message=FALSE,warning=FALSE}
# Data distribution
plot_num(data, bins=10) 
```

Distribution of variables for each target value

```{r Target correlation with predictors,echo=FALSE,message=FALSE,warning=FALSE}
# Target correlation with predictors
# Plot and facet wrap density plots
data %>% 
  gather("feature", "value", -target) %>%
  ggplot(aes(value, fill = target)) +
  geom_density(alpha = 0.5) +
  xlab("Feature values") +
  ylab("Density") +
  theme(legend.position = "top",
        axis.text.x = element_blank(), axis.text.y = element_blank(),
        legend.title=element_blank()) +
  scale_fill_discrete(labels = c("No", "Yes")) +
  facet_wrap(~ feature, scales = "free", ncol = 3)
```


\newpage

### Principal Component Analysis

Now we apply the Principal Component Analysis to understand if it is possible 
to reduce the number of predictors


```{r Principal Component Analysis (PCA),echo=FALSE,message=FALSE,warning=FALSE}
# Principal Component Analysis (PCA)
if(!require(funModeling)) install.packages("funModeling", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(rstudioapi)) install.packages("rstudioapi", repos = "http://cran.us.r-project.org")
if(!require(adabag)) install.packages("adabag", repos = "http://cran.us.r-project.org")
if(!require(plyr)) install.packages("plyr", repos = "http://cran.us.r-project.org")
if(!require(MASS)) install.packages("MASS", repos = "http://cran.us.r-project.org")
if(!require(gbm)) install.packages("gbm", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(rpart.plot)) install.packages("rpart.plot")
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
if(!require(ggthemes)) install.packages("ggthemes", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(e1071)) install.packages("e1071")

library(funModeling)
library(caret)
library(rstudioapi)
library(adabag) 
library(plyr) 
library(MASS)
library(gbm)
library(pROC)
library(rpart.plot)
library(knitr)
library(nnet)
library(ggthemes)
library(randomForest)
library(e1071)

pca <- prcomp(data[,1:ncol(data) - 1], center = TRUE, scale = TRUE)
plot(pca, type="l")

# Summary of data after PCA
summary(pca)

# We need 11 variables to reach 95% of the variance

pca_df <- as.data.frame(pca$x)
ggplot(pca_df, aes(x=PC1, y=PC2, col=data$target)) + geom_point(alpha=0.5)

# The data of the first 2 components cannot be easly separated into two classes.
```

The data of the first 2 components cannot be easly separated into two classes.

We need 11 variables to reach 95% of the variance, so there is no point to 
implement PCA to reduce the number of predictors.

We are going to use all predictors on dataset.

\newpage

# **Results**

We begin the implementation of the various algorithms described above.

For the implementation of the algorithms we will use the Caret package

We start creating the partition 80% and 20% to create respectively the 
training dataset and test dataset.
```{r Set training,echo=FALSE,message=FALSE,warning=FALSE}
# Creation of the partition 80% and 20%
set.seed(1, sample.kind="Rounding")
target_index <- createDataPartition(data$target, times=1, p=0.8, list = FALSE)
train_data <- data[target_index, ]
test_data <- data[-target_index, ]
```


```{r Define train control,echo=FALSE,message=FALSE,warning=FALSE }
#The function trainControl generates parameters that further control how models are created
# Define train control 
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)
```

For each algorithm, the following operations will be performed:

1. set up a grid of adjustment parameters which, by adapting to the model and 
   calculating its performance, will allow us to determine the values that 
   provide optimal performance;
2. Train model;
3. Exploration and visualization of the model, with indication of the best 
   values for tuning parameters;
4. Perform the prediction;
5. Evaluation of confusion matrix will show the best values for parameters and 
   the relative performance obtained;
6. Show the 10 most important predictors;
7. Show the ROC cure with highlighted the best value for Specificity 
   and Sensitivity.
   
\newpage
## Support Vector Machine

```{r Set up grid for Support Vector Machine,echo=FALSE,message=FALSE,warning=FALSE}
# Set up tuning grid
svmRadial_grid <- expand.grid(C = seq(0.1, 2, length = 20), sigma = 0.5)
```

Tuning parameters:
1. sigma (Sigma)
2. C (Cost)

```{r Train Support Vector Machine,echo=FALSE,message=FALSE,warning=FALSE}
# Train model
svmRadial_grid <- expand.grid(C = seq(0.1, 2, length = 20), sigma = 0.5)

set.seed(1, sample.kind="Rounding")
svmRadial_model <- train(target~., data=train_data,
                   method = "svmRadial", 
                   trControl = fitControl, 
                   verbose = FALSE, 
                   tuneGrid = svmRadial_grid,
                   # center, scale - centering and scaling data
                   preProcess = c("center", "scale"), 
                   metric = "ROC")

plot(svmRadial_model)
```

```{r Prediction with Support Vector Machine,echo=FALSE,message=FALSE,warning=FALSE}
# Predict data
set.seed(1, sample.kind="Rounding")
svmRadial_pred <- predict(svmRadial_model, newdata = test_data)

#Evaluate confusion matrix
svmRadial_confusionMatrix <- confusionMatrix(svmRadial_pred, test_data$target)

svmRadial_confusionMatrix

# Plot 10 most important variables
plot(varImp(svmRadial_model), top=10, main="Top variables Support Vector Machine")
```

serum creatinine, ejection fraction and age are the most important

\newpage

ROC curve for Support Vector Machine

```{r ROC curve for Support Vector Machine,echo=FALSE,message=FALSE,warning=FALSE}
# Define ROC Curve
svmRadial_rocCurve   <- roc(response = test_data$target,
                      predictor = as.numeric(svmRadial_pred),
                      levels = rev(levels(test_data$target)),
                      plot = TRUE, col = "blue", auc = TRUE)

# Plot ROC curve
plot(svmRadial_rocCurve, print.thres = "best")
```

\newpage
## Adaptive Boosting

```{r Set up grid for Adaptive Boosting,echo=FALSE,message=FALSE,warning=FALSE}
# Set up tuning grid
am1_grid <- expand.grid(mfinal = (1:3)*3,         
                         maxdepth = c(1, 3),       
                         coeflearn = c("Zhu"))
```

Tuning parameters:

1. mfinal (#Trees)
2. maxdepth (Max Tree Depth)
3. coeflearn (Coefficient Type)

```{r Train Adaptive Boosting,echo=FALSE,message=FALSE,warning=FALSE}
# Train model
set.seed(1, sample.kind="Rounding")
am1_model <- train(target~., data=train_data,
                   method = "AdaBoost.M1", 
                   trControl = fitControl, 
                   verbose = FALSE, 
                   tuneGrid = am1_grid,
                   # center, scale - centering and scaling data
                   preProcess = c("center", "scale"), 
                   metric = "ROC")

plot(am1_model)
```

```{r Prediction with Adaptive Boosting,echo=FALSE,message=FALSE,warning=FALSE}
# Predict data
set.seed(1, sample.kind="Rounding")
am1_pred <- predict(am1_model, newdata = test_data)

# Evaluate confusion matrix
am1_confusionMatrix <- confusionMatrix(am1_pred, test_data$target)

am1_confusionMatrix

# Plot 10 most important variables
plot(varImp(am1_model), top=10, main="Top variables Adaptive Boosting")
```

serum creatinine, age and ejection fraction are the most important

\newpage

ROC curve for Adaptive Boosting

```{r ROC curve for Adaptive Boosting,echo=FALSE,message=FALSE,warning=FALSE}
# Define ROC Curve
am1_rocCurve   <- roc(response = test_data$target,
                      predictor = as.numeric(am1_pred),
                      levels = rev(levels(test_data$target)),
                      plot = TRUE, col = "blue", auc = TRUE)

# Plot ROC curve
plot(am1_rocCurve, print.thres = "best")
```
\newpage

## Stochastic Gradient Boosting

```{r Set up grid for Stochastic Gradient Boosting,echo=FALSE,message=FALSE,warning=FALSE}
# Set up tuning grid
gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                         n.trees = (1:30)*50, 
                         shrinkage = 0.1,
                         n.minobsinnode = 20)
```

Tuning parameters:

1. interaction.depth (Max Tree Depth)
2. n.trees (Boosting Iterations)
3. shrinkage (Bandwidth Adjustment)
4. n.minobsinnode (Min. Terminal Node Size)

```{r Train Stochastic Gradient Boosting,echo=FALSE,message=FALSE,warning=FALSE}
# Train model
set.seed(1, sample.kind="Rounding")
gbm_model <- train(target~., data=train_data,
                   method = "gbm", 
                   trControl = fitControl, 
                   verbose = FALSE, 
                   tuneGrid = gbm_grid,
                   # center, scale - centering and scaling data
                   preProcess = c("center", "scale"), 
                   ## Specify which metric to optimize
                   metric = "ROC")

plot(gbm_model)
```

```{r Prediction with Stochastic Gradient Boosting,echo=FALSE,message=FALSE,warning=FALSE}
# Predict data
set.seed(1, sample.kind="Rounding")
gbm_pred <- predict(gbm_model, newdata = test_data)

# Evaluate confusion matrix
gbm_confusionMatrix <- confusionMatrix(gbm_pred, test_data$target)

gbm_confusionMatrix

# Plot 10 most important variables
plot(varImp(gbm_model), top=10, main="Top variables Stochastic Gradient Boosting")
```

serum creatinine, ejection fraction and age are the most important

\newpage

ROC curve for Stochastic Gradient Boosting

```{r ROC curve for Stochastic Gradient Boosting,echo=FALSE,message=FALSE,warning=FALSE}
# Define ROC Curve
gbm_rocCurve   <- roc(response = test_data$target,
                      predictor = as.numeric(gbm_pred),
                      levels = rev(levels(test_data$target)),
                      plot = TRUE, col = "blue", auc = TRUE)

# Plot ROC curve
plot(gbm_rocCurve, print.thres = "best")
```
\newpage

## Classification Trees

```{r Set up grid for Classification Trees,echo=FALSE,message=FALSE,warning=FALSE}
# Set up tuning grid
ct_grid <-  data.frame(cp = seq(0.0, 0.1, len = 25))
```

Tuning parameters:

1. cp (Complexity Parameter)

```{r Train Classification Trees,echo=FALSE,message=FALSE,warning=FALSE}
# Train model
set.seed(1, sample.kind="Rounding")
ct_model <- train(target~., data=train_data,
                 method = "rpart",
                 metric="ROC",
                 # center, scale - centering and scaling data
                 preProcess = c("center", "scale"), 
                 tuneGrid = ct_grid,
                 trControl = fitControl)

plot(ct_model)
```

```{r Prediction with Classification Trees,echo=FALSE,message=FALSE,warning=FALSE}
# Predict data
set.seed(1, sample.kind="Rounding")
ct_pred <- predict(ct_model, newdata = test_data)

# Evaluate confusion matrix
ct_confusionMatrix <- confusionMatrix(ct_pred, test_data$target)

ct_confusionMatrix

# Plot 10 most important variables
plot(varImp(ct_model), top=10, main="Top variables Classification Tree")
```

age, serum creatinine and ejection fraction are the most important

\newpage

ROC curve for Classification Trees

```{r ROC curve for Classification Trees,echo=FALSE,message=FALSE,warning=FALSE}
# Define ROC Curve
ct_rocCurve   <- roc(response = test_data$target,
                    predictor = as.numeric(ct_pred),
                    levels = rev(levels(test_data$target)),
                    plot = TRUE, col = "blue", auc = TRUE)

# Plot ROC curve
plot(ct_rocCurve, print.thres = "best")
```

The graph below shows the decision tree

```{r Classification Trees,echo=FALSE,message=FALSE,warning=FALSE}
rpart.plot(ct_model$finalModel)
```
\newpage

## Random Forest

```{r Set up grid for Random Forest,echo=FALSE,message=FALSE,warning=FALSE}
# Set up tuning grid
rf_grid <-  data.frame(mtry = seq(1, 10))
```

Tuning parameters:

1. mtry (#Randomly Selected Predictors)

```{r Train Random Forest,echo=FALSE,message=FALSE,warning=FALSE}
# Train model
set.seed(1, sample.kind="Rounding")
rf_model <- train(target~., data=train_data,
                  method = "rf",
                  metric = "ROC",
                  # center, scale - centering and scaling data
                  preProcess = c("center", "scale"), 
                  tuneGrid = rf_grid,
                  ntree = 100,
                  trControl = fitControl)

plot(rf_model)
```

```{r Prediction with Random Forest,echo=FALSE,message=FALSE,warning=FALSE}
# Predict data
set.seed(1, sample.kind="Rounding")
rf_pred <- predict(rf_model, newdata = test_data)

# Evaluate confusion matrix
rf_confusionMatrix <- confusionMatrix(rf_pred, test_data$target)

rf_confusionMatrix

# Plot 10 most important variables
plot(varImp(rf_model), top=10, main="Top variables Random Forest")
```

serum creatinine, ejection fraction and age are the most important

\newpage

ROC curve for Random Forest

```{r ROC curve for Random Forest,echo=FALSE,message=FALSE,warning=FALSE}
# Define ROC Curve
rf_rocCurve   <- roc(response = test_data$target,
                  predictor = as.numeric(rf_pred),
                  levels = rev(levels(test_data$target)),
                  plot = TRUE, col = "blue", auc = TRUE)

# Plot ROC curve
plot(rf_rocCurve, print.thres = "best")
```
\newpage

## K Nearest Neighbor (KNN)

```{r Set up grid for K Nearest Neighbor,echo=FALSE,message=FALSE,warning=FALSE}
# Set up tuning grid
knn_grid <-  data.frame(k = seq(1, 50, 1))
```

Tuning parameters:

1. k (#Neighbors)

```{r Train K Nearest Neighbor,echo=FALSE,message=FALSE,warning=FALSE}
# Train model
set.seed(1, sample.kind="Rounding")
knn_model <- train(target~., data=train_data,
                   method="knn",
                   metric="ROC",
                   # center, scale - centering and scaling data
                   preProcess = c("center", "scale"), 
                   tuneGrid = knn_grid,
                   trControl=fitControl)

plot(knn_model)
```

```{r Prediction with K Nearest Neighbor,echo=FALSE,message=FALSE,warning=FALSE}
# Predict data
set.seed(1, sample.kind="Rounding")
knn_pred <- predict(knn_model, newdata = test_data)

# Evaluate confusion matrix
knn_confusionMatrix <- confusionMatrix(knn_pred, test_data$target)

knn_confusionMatrix

# Plot 10 most important variables
plot(varImp(knn_model), top=10, main="Top variables K Nearest Neighbor")
```

serum creatinine, ejection fraction and age are the most important

\newpage

ROC curve for K Nearest Neighbor

```{r ROC curve for K Nearest Neighbor,echo=FALSE,message=FALSE,warning=FALSE}
# Define ROC Curve
knn_rocCurve   <- roc(response = test_data$target,
                      predictor = as.numeric(knn_pred),
                      levels = rev(levels(test_data$target)),
                      plot = TRUE, col = "blue", auc = TRUE)

# Plot ROC curve
plot(knn_rocCurve, print.thres = "best")
```
\newpage

## Neural Network

```{r Set up grid for Neural Network,echo=FALSE,message=FALSE,warning=FALSE}
# Set up tuning grid
nn_grid <- expand.grid(size = c(1:5, 10),
                       decay = c(0, 0.05, 0.1, 1, 2))
```

Tuning parameters:

1. size (#Hidden Units)
2. decay (Weight Decay)

```{r Train Neural Network,echo=FALSE,message=FALSE,warning=FALSE}
# Train model
set.seed(1, sample.kind="Rounding")
nn_model <- train(target~., data=train_data,
                  method="nnet",
                  metric="ROC",
                  # center, scale - centering and scaling data
                  preProcess = c("center", "scale"), 
                  tuneGrid = nn_grid,
                  trace=FALSE,
                  trControl=fitControl)

plot(nn_model)
```

```{r Prediction with Neural Network,echo=FALSE,message=FALSE,warning=FALSE}
# Predict data
set.seed(1, sample.kind="Rounding")
nn_pred <- predict(nn_model, newdata = test_data)

# Evaluate confusion matrix
nn_confusionMatrix <- confusionMatrix(nn_pred, test_data$target)

nn_confusionMatrix

# Plot 10 most important variables
plot(varImp(nn_model), top=10, main="Top variables Neural Network Model")
```

ejection fraction, serum creatinine and age are the most important

\newpage

ROC curve for Neural Network

```{r ROC curve for Neural Network,echo=FALSE,message=FALSE,warning=FALSE}
# Define ROC Curve
nn_rocCurve   <- roc(response = test_data$target,
                      predictor = as.numeric(nn_pred),
                      levels = rev(levels(test_data$target)),
                      plot = TRUE, col = "blue", auc = TRUE)

# Plot ROC curve
plot(nn_rocCurve, print.thres = "best")
```
\newpage

## Compare algorithms
In this last phase, are compared the results produced by the various algorithms 
```{r Compare methods,echo=FALSE,message=FALSE,warning=FALSE}
#List of all algorithms
models_list <- list(SVM = svmRadial_model,
                    ADP_Boost = am1_model,
                    Gradient_Bost=gbm_model,
                    Class_Tree = ct_model,
                    RND_Forest=rf_model,
                    KNN=knn_model,
                    NN=nn_model) 

models_results <- resamples(models_list)

# Summary of algorithms
summary(models_results)

# Confusion matrix of the algorithms
confusion_matrix_list <- list(
  SVM = svmRadial_confusionMatrix,
  ADP_Boost=am1_confusionMatrix, 
  Gradient_Bost=gbm_confusionMatrix,
  Class_Tree = ct_confusionMatrix,
  RND_Forest=rf_confusionMatrix,
  KNN=knn_confusionMatrix,
  NN=nn_confusionMatrix) 

confusion_matrix_list_results <- sapply(confusion_matrix_list, function(x) x$byClass)
confusion_matrix_list_results %>% kable()
```
***K Nearest Neighbor*** algorithm has been identified as the best one 
for ***sensitivity***, instead ***Classification Tree*** is the best for ***specificity***

\newpage

We use again the classification model, but considering only serum creatinine, 
ejection fraction and age as variables

```{r Machine learning with serum creatinine and ejection fraction,echo=FALSE,message=FALSE,warning=FALSE}
data <- data %>% dplyr::select(serum_sodium, ejection_fraction, age, target)


# Start training data

# Creation of the partition 80% and 20%
set.seed(1, sample.kind="Rounding")
target_index <- createDataPartition(data$target, times=1, p=0.8, list = FALSE)
train_data <- data[target_index, ]
test_data <- data[-target_index, ]

# Define train control 
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)


# Support Vector Machine
svmRadial_grid <- expand.grid(C = seq(0.1, 2, length = 20), sigma = 0.5)

set.seed(1, sample.kind="Rounding")
svmRadial_model <- train(target~., data=train_data,
                         method = "svmRadial", 
                         trControl = fitControl, 
                         verbose = FALSE, 
                         tuneGrid = svmRadial_grid,
                         # center, scale - centering and scaling data
                         preProcess = c("center", "scale"), 
                         metric = "ROC")

set.seed(1, sample.kind="Rounding")
svmRadial_pred <- predict(svmRadial_model, newdata = test_data)

#Evaluate confusion matrix
svmRadial_confusionMatrix <- confusionMatrix(svmRadial_pred, test_data$target)

# Adaptive Boosting
am1_grid <- expand.grid(mfinal = (1:3)*3,         
                        maxdepth = c(1, 3),       
                        coeflearn = c("Zhu"))

# Train model
set.seed(1, sample.kind="Rounding")
am1_model <- train(target~., data=train_data,
                   method = "AdaBoost.M1", 
                   trControl = fitControl, 
                   verbose = FALSE, 
                   tuneGrid = am1_grid,
                   # center, scale - centering and scaling data
                   preProcess = c("center", "scale"), 
                   metric = "ROC")

set.seed(1, sample.kind="Rounding")
am1_pred <- predict(am1_model, newdata = test_data)

# Evaluate confusion matrix
am1_confusionMatrix <- confusionMatrix(am1_pred, test_data$target)

# Stochastic Gradient Boosting
gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                         n.trees = (1:30)*50, 
                         shrinkage = 0.1,
                         n.minobsinnode = 20)

set.seed(1, sample.kind="Rounding")
gbm_model <- train(target~., data=train_data,
                   method = "gbm", 
                   trControl = fitControl, 
                   verbose = FALSE, 
                   tuneGrid = gbm_grid,
                   # center, scale - centering and scaling data
                   preProcess = c("center", "scale"), 
                   metric = "ROC")

set.seed(1, sample.kind="Rounding")
gbm_pred <- predict(gbm_model, newdata = test_data)

# Evaluate confusion matrix
gbm_confusionMatrix <- confusionMatrix(gbm_pred, test_data$target)

# Classification Trees
ct_grid <-  data.frame(cp = seq(0.0, 0.1, len = 25))

set.seed(1, sample.kind="Rounding")
ct_model <- train(target~., data=train_data,
                  method = "rpart",
                  metric="ROC",
                  # center, scale - centering and scaling data
                  preProcess = c("center", "scale"), 
                  tuneGrid = ct_grid,
                  trControl = fitControl)

set.seed(1, sample.kind="Rounding")
ct_pred <- predict(ct_model, newdata = test_data)

# Evaluate confusion matrix
ct_confusionMatrix <- confusionMatrix(ct_pred, test_data$target)

# Random Forest
rf_grid <-  data.frame(mtry = seq(1, 10))

set.seed(1, sample.kind="Rounding")
rf_model <- train(target~., data=train_data,
                  method = "rf",
                  metric = "ROC",
                  # center, scale - centering and scaling data
                  preProcess = c("center", "scale"), 
                  tuneGrid = rf_grid,
                  ntree = 100,
                  trControl = fitControl)

set.seed(1, sample.kind="Rounding")
rf_pred <- predict(rf_model, newdata = test_data)

# Evaluate confusion matrix
rf_confusionMatrix <- confusionMatrix(rf_pred, test_data$target)

# K Nearest Neighbor (KNN)
knn_grid <-  data.frame(k = seq(1, 50, 1))

set.seed(1, sample.kind="Rounding")
knn_model <- train(target~., data=train_data,
                   method="knn",
                   metric="ROC",
                   # center, scale - centering and scaling data
                   preProcess = c("center", "scale"), 
                   tuneGrid = knn_grid,
                   trControl=fitControl)

set.seed(1, sample.kind="Rounding")
knn_pred <- predict(knn_model, newdata = test_data)

# Evaluate confusion matrix
knn_confusionMatrix <- confusionMatrix(knn_pred, test_data$target)

# Neural Network
nn_grid <- expand.grid(size = c(1:5, 10),
                       decay = c(0, 0.05, 0.1, 1, 2))

set.seed(1, sample.kind="Rounding")
nn_model <- train(target~., data=train_data,
                  method="nnet",
                  metric="ROC",
                  # center, scale - centering and scaling data
                  preProcess = c("center", "scale"), 
                  tuneGrid = nn_grid,
                  trace=FALSE,
                  trControl=fitControl)

set.seed(1, sample.kind="Rounding")
nn_pred <- predict(nn_model, newdata = test_data)

# Evaluate confusion matrix
nn_confusionMatrix <- confusionMatrix(nn_pred, test_data$target)


# Compare algorithms
models_list <- list(SVM = svmRadial_model,
                    ADP_Boost = am1_model,
                    Gradient_Bost=gbm_model,
                    Class_Tree = ct_model,
                    RND_Forest=rf_model,
                    KNN=knn_model,
                    NN=nn_model) 

models_results <- resamples(models_list)

# Summary of algorithms
summary(models_results)

# Confusion matrix of the algorithms
confusion_matrix_list <- list(
  SVM = svmRadial_confusionMatrix,
  ADP_Boost=am1_confusionMatrix, 
  Gradient_Bost=gbm_confusionMatrix,
  Class_Tree = ct_confusionMatrix,
  RND_Forest=rf_confusionMatrix,
  KNN=knn_confusionMatrix,
  NN=nn_confusionMatrix) 

confusion_matrix_list_results <- sapply(confusion_matrix_list, function(x) x$byClass)
confusion_matrix_list_results %>% kable()
```
***K Nearest Neighbor*** and ***Neural Network*** algorithms are the best one 
for ***sensitivity***, instead ***Adaptive Boosting*** is the best for ***specificity***
With only three variables the ***sensitivity*** doesn't change. Instead for 
***specificity*** we have lower performance

\newpage

# **Conclusion**
The objective of this project was to analyze the Heart failure dataset to build 
a classifiers to predict the heart failure and what are the most important variables. 
In particular was compared the ***classification*** capacity of the algorithms, with particular regard to the topic of ***specificity*** and ***sensitivity***.

We started by analyzing the dataset to understand the structure of the data and in particular was analyzed the correlation between predictors and the possibility of reducing their number

We then selected ROC as the metric to compare algorithms

We identified the algorithms to be implemented to create the system and then evaluated their quality in terms of ***sensitivity*** and ***specificity***.

For each algorithm we evaluate the most important predictors.

***K Nearest Neighbor*** algorithm has been identified as the best one for ***sensitivity***, instead ***Classification Tree*** is the best performance for ***specificity***.

The most important variables are: serum creatinine, ejection fraction and age.

We then repeated the construction of the classification model using a dataset reduced to its three most important variables.

***K Nearest Neighbor*** and ***Neural Network*** algorithms are the best one for ***sensitivity***, instead ***Adaptive Boosting*** is the best performance for ***specificity***.

With only three variables the ***sensitivity*** doesn't change. Instead for ***specificity*** we have lower performance.

The present project does not represent an exhaustive analysis, to obtain this result should be considered the possible variants of the implemented algorithms, as well as the opportunity to implement other algorithms among those available for classification problems.

Last but not least, it would be possible to use a dataset with many more observations, as the dataset used contains only 299 observations.